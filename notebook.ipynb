{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "z8o3HBAmspak"
      },
      "outputs": [],
      "source": [
        "#!wget https://dataworks.indianapolis.iu.edu/bitstream/handle/11243/41/data.zip\n",
        "#!unzip -q data.zip\n",
        "#!rm data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTwIpTTYsaIR",
        "outputId": "1d33d159-8db0-4c0a-c042-311c07b15310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import models, transforms, datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from functools import reduce\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature extraction\n",
        "Following the reference paper, we extract the features using a ResNet-101 pretrained on ImageNet-1k. The transformations include resizing the images to $[256\\times 256\\times 3]$, centre-­cropping the resized images to $[224\\times 224\\times 3]$ and finally normalizing the resized images by colour channel using the ImageNet image pixel means $[0.485, 0.456, 0.406]$ and standard deviations $[0.229, 0.224, 0.225]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 217/217 [01:54<00:00,  1.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.6911849  2.2810183  0.685512   ... 1.455803   0.16906478 0.13312015]\n",
            " [0.7539211  1.9619342  0.36932412 ... 1.1854475  0.4905457  0.1034935 ]\n",
            " [0.44997358 1.7392105  0.6140515  ... 0.98125035 0.10975925 0.07306477]\n",
            " ...\n",
            " [0.9099493  2.7838514  0.2924271  ... 1.7474841  0.32095304 0.1183937 ]\n",
            " [0.78014404 0.9348191  0.88631487 ... 2.5543966  0.34553403 0.360478  ]\n",
            " [0.9932794  1.853858   0.51197165 ... 0.8813355  0.33022955 0.17072845]]\n",
            "6917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder('data/INSECTS_Images_2021', transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "feature_extractor = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
        "feature_extractor.to(device)\n",
        "feature_extractor.eval()\n",
        "\n",
        "# Remove the final fully connected layer to get the feature vector\n",
        "feature_extractor = torch.nn.Sequential(*(list(feature_extractor.children())[:-1]))\n",
        "\n",
        "features = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in tqdm(dataloader):\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = feature_extractor(inputs)\n",
        "        features.append(outputs.squeeze().cpu().numpy())\n",
        "\n",
        "features = np.concatenate(features, axis=0)\n",
        "print(features)\n",
        "print(len(features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.6911, 2.2810, 0.6855,  ..., 1.4559, 0.1691, 0.1331],\n",
            "        [0.7539, 1.9619, 0.3693,  ..., 1.1854, 0.4905, 0.1035],\n",
            "        [0.4497, 1.7506, 0.6316,  ..., 0.9740, 0.1137, 0.0703],\n",
            "        ...,\n",
            "        [0.5839, 1.9473, 0.1817,  ..., 0.5017, 2.6092, 0.1645],\n",
            "        [0.4488, 1.4840, 1.0447,  ..., 0.2559, 0.6805, 0.2374],\n",
            "        [1.7684, 1.1025, 1.4147,  ..., 0.9506, 2.3582, 0.3438]])\n"
          ]
        }
      ],
      "source": [
        "data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "print(torch.from_numpy(data_mat[\"embeddings_img\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pc1BZG2saIW"
      },
      "source": [
        "# Dataset import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "n4jHfAYRsaIY"
      },
      "outputs": [],
      "source": [
        "class ImageDNATrainDataset():\n",
        "    def __init__(self, train=True):\n",
        "        splits_mat = scipy.io.loadmat(\"data/INSECTS/splits.mat\")\n",
        "        train_loc = splits_mat[\"train_loc\"]-1\n",
        "\n",
        "        TRAINING_SAMPLES_NUMBER = 12481\n",
        "        TRAINING_LABELS_NUMBER = 652\n",
        "\n",
        "        assert len(train_loc[0]) == TRAINING_SAMPLES_NUMBER\n",
        "\n",
        "        indeces = train_loc\n",
        "        # indeces.shape is (1, |indeces|), so we extract the whole list using [0]\n",
        "        indeces = indeces[0]\n",
        "\n",
        "        data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "        self.embeddings_img = torch.from_numpy(\n",
        "            data_mat[\"embeddings_img\"][indeces]\n",
        "        ).float()\n",
        "        self.embeddings_dna = torch.from_numpy(\n",
        "            data_mat[\"embeddings_dna\"][indeces]\n",
        "        ).float()\n",
        "\n",
        "        # Remap seen species in [0, 651]\n",
        "        seen_species = data_mat[\"labels\"][train_loc][0]\n",
        "        seen_species_mapping = {label: i for i, label in enumerate(np.unique(seen_species))}\n",
        "\n",
        "        species_mapping = seen_species_mapping\n",
        "        assert len(species_mapping) == TRAINING_LABELS_NUMBER\n",
        "\n",
        "        species = data_mat[\"labels\"][indeces]\n",
        "        remapped_species = np.array([species_mapping[label.item()] for label in species])\n",
        "        self.remapped_species = torch.from_numpy(remapped_species).long()\n",
        "\n",
        "        assert len(torch.unique(self.remapped_species)) == TRAINING_LABELS_NUMBER\n",
        "\n",
        "        # data_mat['G'] returns a ndarray of type uint16, therefore we convert into int16 before invoking from_numpy\n",
        "        self.G = torch.from_numpy(data_mat[\"G\"].astype(np.int16)).long()\n",
        "        self.genera = torch.empty(species.shape).long()\n",
        "        for i in range(indeces.size):\n",
        "            self.genera[i][0] = self.G[species[i][0] - 1][0] - 1041\n",
        "\n",
        "        assert len(self.genera) == TRAINING_SAMPLES_NUMBER\n",
        "\n",
        "        self.species_names = data_mat[\"species\"][indeces]\n",
        "        self.ids = data_mat[\"ids\"][indeces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings_dna)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding_img = self.embeddings_img[idx]\n",
        "        embedding_dna = self.embeddings_dna[idx]\n",
        "        label = self.remapped_species[idx].item()\n",
        "        genera = self.genera[idx].item()\n",
        "\n",
        "        return embedding_img.view(1, -1), embedding_dna.view(1, -1), label, genera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "GasJN0zyusZi"
      },
      "outputs": [],
      "source": [
        "class ImageDNAValidationDataset():\n",
        "    def __init__(self, train=True):\n",
        "        splits_mat = scipy.io.loadmat(\"data/INSECTS/splits.mat\")\n",
        "        train_loc = splits_mat[\"train_loc\"]-1\n",
        "        val_seen_loc = splits_mat[\"val_seen_loc\"]-1\n",
        "        val_unseen_loc = splits_mat[\"val_unseen_loc\"]-1\n",
        "\n",
        "        TRAINING_LABELS_NUMBER = 652\n",
        "        VALIDATION_SAMPLES_NUMBER = 6939\n",
        "        VALIDATION_SPECIES_NUMBER = 774\n",
        "        TRAINING_VALIDATION_SPECIES_NUMBER = 797\n",
        "        VALIDATION_SEEN_SPECIES_NUMBER = 629\n",
        "        VALIDATION_UNSEEN_SPECIES_GENERA_NUMBER = 97\n",
        "\n",
        "        indeces = np.concatenate((val_seen_loc, val_unseen_loc), axis=1)\n",
        "        # indeces.shape is (1, |indeces|), so we extract the whole list using [0]\n",
        "        indeces = indeces[0]\n",
        "        assert len(indeces) == VALIDATION_SAMPLES_NUMBER\n",
        "\n",
        "        data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "        self.embeddings_img = torch.from_numpy(data_mat[\"embeddings_img\"][indeces]).float()\n",
        "        self.embeddings_dna = torch.from_numpy(data_mat[\"embeddings_dna\"][indeces]).float()\n",
        "\n",
        "        # Remap seen species in [0, 651]\n",
        "        seen_species = data_mat[\"labels\"][train_loc][0]\n",
        "        seen_species_mapping = {label: i for i, label in enumerate(np.unique(seen_species))}\n",
        "\n",
        "        # Remap unseen species during validation in [652, 796]\n",
        "        unseen_species = data_mat[\"labels\"][val_unseen_loc][0]\n",
        "        unseen_species_mapping = {label: i + TRAINING_LABELS_NUMBER for i, label in enumerate(np.unique(unseen_species))}\n",
        "\n",
        "        # Union of the two mappings, allows to fully remap all the labels\n",
        "        species_mapping = seen_species_mapping | unseen_species_mapping\n",
        "        assert len(species_mapping) == TRAINING_VALIDATION_SPECIES_NUMBER\n",
        "\n",
        "        species = data_mat[\"labels\"][indeces]\n",
        "        remapped_species = np.array([species_mapping[label.item()] for label in species])\n",
        "        self.remapped_species = torch.from_numpy(remapped_species).long()\n",
        "        assert len(torch.unique(self.remapped_species)) == VALIDATION_SPECIES_NUMBER\n",
        "\n",
        "        # data_mat['G'] returns a ndarray of type uint16, therefore we convert into int16 before invoking from_numpy\n",
        "        self.G = torch.from_numpy(data_mat[\"G\"].astype(np.int16)).long()\n",
        "        self.genera = torch.empty(species.shape).long()\n",
        "        for i in range(indeces.size):\n",
        "            self.genera[i][0] = self.G[species[i][0] - 1][0] - 1041\n",
        "\n",
        "        # Compute genera of unseen species in the validation set\n",
        "        unseen_species_genera = []\n",
        "        for i in val_unseen_loc[0]:\n",
        "            unseen_species_genera.append(data_mat[\"G\"][data_mat[\"labels\"][i][0] - 1][0] - 1041)\n",
        "        self.unseen_species_genera = np.array(unseen_species_genera)\n",
        "        assert len(np.unique(self.unseen_species_genera)) == VALIDATION_UNSEEN_SPECIES_GENERA_NUMBER\n",
        "\n",
        "        # Compute seen species number in the validation set\n",
        "        seen_species = []\n",
        "        for i in val_seen_loc[0]:\n",
        "            seen_species.append(species_mapping[data_mat[\"labels\"][i].item()])\n",
        "        self.seen_species = np.array(seen_species)\n",
        "        assert len(np.unique(self.seen_species)) == VALIDATION_SEEN_SPECIES_NUMBER\n",
        "\n",
        "        self.species_names = data_mat[\"species\"][indeces]\n",
        "        self.ids = data_mat[\"ids\"][indeces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings_dna)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding_img = self.embeddings_img[idx]\n",
        "        embedding_dna = self.embeddings_dna[idx]\n",
        "        label = self.remapped_species[idx].item()\n",
        "        genera = self.genera[idx].item()\n",
        "\n",
        "        return embedding_img.view(1, -1), embedding_dna.view(1, -1), label, genera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "MKdIHeWZusZj"
      },
      "outputs": [],
      "source": [
        "class ImageDNATestDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        splits_mat = scipy.io.loadmat(\"data/INSECTS/splits.mat\")\n",
        "        train_loc = splits_mat[\"train_loc\"]-1\n",
        "        trainval_loc = splits_mat[\"trainval_loc\"]-1\n",
        "        test_seen_loc = splits_mat[\"test_seen_loc\"]-1\n",
        "        test_unseen_loc = splits_mat[\"test_unseen_loc\"]-1\n",
        "        val_seen_loc = splits_mat[\"val_seen_loc\"]-1\n",
        "        val_unseen_loc = splits_mat[\"val_unseen_loc\"]-1\n",
        "\n",
        "        TRAINING_SPECIES_NUMBER = 652\n",
        "        TRAINING_VALIDATION_SPECIES = 797\n",
        "        NUMBER_OF_SPECIES = 1040\n",
        "        TEST_SEEN_SPECIES_NUMBER = 770\n",
        "        TEST_UNSEEN_SPECIES_GENERA_NUMBER = 134\n",
        "\n",
        "        indeces = np.concatenate((test_seen_loc, test_unseen_loc), axis=1)\n",
        "        # indeces.shape is (1, |indeces|), so we extract the whole list using [0]\n",
        "        indeces = indeces[0]\n",
        "\n",
        "        data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "        self.embeddings_img = torch.from_numpy(data_mat[\"embeddings_img\"][indeces]).float()\n",
        "        self.embeddings_dna = torch.from_numpy(data_mat[\"embeddings_dna\"][indeces]).float()\n",
        "\n",
        "        # Remap seen species in [0, 651]\n",
        "        seen_species = data_mat[\"labels\"][train_loc][0]\n",
        "        seen_species_mapping = {label: i for i, label in enumerate(np.unique(seen_species))}\n",
        "\n",
        "        # Remap unseen species during validation in [652, 796]\n",
        "        unseen_species_validation = data_mat[\"labels\"][val_unseen_loc][0]\n",
        "        unseen_species_validation_mapping = {label: i + TRAINING_SPECIES_NUMBER for i, label in enumerate(np.unique(unseen_species_validation))}\n",
        "\n",
        "        # Remap unseen species during test in [797, 1039]\n",
        "        unseen_species_test = data_mat[\"labels\"][test_unseen_loc][0]\n",
        "        unseen_species_test_mapping = {label: i + TRAINING_VALIDATION_SPECIES for i, label in enumerate(np.unique(unseen_species_test))}\n",
        "\n",
        "        assert reduce(np.intersect1d, (seen_species, unseen_species_validation, unseen_species_test)).size == 0\n",
        "\n",
        "        # Union of the two mappings, allows to full remap all the labels\n",
        "        labels_mapping = seen_species_mapping | unseen_species_validation_mapping | unseen_species_test_mapping\n",
        "        assert len(labels_mapping) == NUMBER_OF_SPECIES\n",
        "\n",
        "        species = data_mat[\"labels\"][indeces]\n",
        "        remapped_labels = np.array([labels_mapping[label.item()] for label in species])\n",
        "        self.remapped_labels = torch.from_numpy(remapped_labels).long()\n",
        "\n",
        "        # data_mat['G'] returns a ndarray of type uint16, therefore we convert into int16 before invoking from_numpy\n",
        "        self.G = torch.from_numpy(data_mat[\"G\"].astype(np.int16)).long()\n",
        "        self.genera = torch.empty(species.shape).long()\n",
        "        for i in range(indeces.size):\n",
        "            self.genera[i][0] = self.G[species[i][0] - 1][0] - 1041\n",
        "\n",
        "        # Compute genera of unseen species\n",
        "        unseen_species_genera = []\n",
        "        for i in test_unseen_loc[0]:\n",
        "            unseen_species_genera.append(data_mat[\"G\"][data_mat[\"labels\"][i][0] - 1][0] - 1041)\n",
        "\n",
        "        self.unseen_species_genera = np.array(unseen_species_genera)\n",
        "        assert len(np.unique(self.unseen_species_genera)) == TEST_UNSEEN_SPECIES_GENERA_NUMBER\n",
        "\n",
        "        # Compute seen species\n",
        "        seen_species = []\n",
        "        for i in test_seen_loc[0]:\n",
        "            seen_species.append(labels_mapping[data_mat[\"labels\"][i].item()])\n",
        "        self.seen_species = np.array(seen_species)\n",
        "        assert len(np.unique(self.seen_species)) == TEST_SEEN_SPECIES_NUMBER\n",
        "\n",
        "        self.species_name = data_mat[\"species\"][indeces]\n",
        "        self.ids = data_mat[\"ids\"][indeces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings_dna)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding_img = self.embeddings_img[idx]\n",
        "        embedding_dna = self.embeddings_dna[idx]\n",
        "        label = self.remapped_labels[idx].item()\n",
        "        genera = self.genera[idx].item()\n",
        "\n",
        "        return embedding_img.view(1, -1), embedding_dna.view(1, -1), label, genera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "PWLG3L3eusZl"
      },
      "outputs": [],
      "source": [
        "class ImageDNATrainValidationDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        splits_mat = scipy.io.loadmat(\"data/INSECTS/splits.mat\")\n",
        "        train_loc = splits_mat[\"train_loc\"]-1\n",
        "        trainval_loc = splits_mat[\"trainval_loc\"]-1\n",
        "        test_seen_loc = splits_mat[\"test_seen_loc\"]-1\n",
        "        test_unseen_loc = splits_mat[\"test_unseen_loc\"]-1\n",
        "        val_seen_loc = splits_mat[\"val_seen_loc\"]-1\n",
        "        val_unseen_loc = splits_mat[\"val_unseen_loc\"]-1\n",
        "\n",
        "        TRAINING_SPECIES_NUMBER = 652\n",
        "        TRAINING_VALIDATION_SPECIES = 797\n",
        "        NUMBER_OF_SPECIES = 1040\n",
        "\n",
        "        indeces = trainval_loc\n",
        "        # indeces.shape is (1, |indeces|), so we extract the whole list using [0]\n",
        "        indeces = indeces[0]\n",
        "\n",
        "        data_mat = scipy.io.loadmat(\"data/INSECTS/data.mat\")\n",
        "        self.embeddings_img = torch.from_numpy(data_mat[\"embeddings_img\"][indeces]).float()\n",
        "        self.embeddings_dna = torch.from_numpy(data_mat[\"embeddings_dna\"][indeces]).float()\n",
        "\n",
        "        # Remap seen species in [0, 651]\n",
        "        seen_species = data_mat[\"labels\"][train_loc][0]\n",
        "        seen_species_mapping = {label: i for i, label in enumerate(np.unique(seen_species))}\n",
        "\n",
        "        # Remap unseen species during validation in [652, 796]\n",
        "        unseen_species_validation = data_mat[\"labels\"][val_unseen_loc][0]\n",
        "        unseen_species_validation_mapping = {label: i + TRAINING_SPECIES_NUMBER for i, label in enumerate(np.unique(unseen_species_validation))}\n",
        "\n",
        "        # Remap unseen species during test in [797, 1039]\n",
        "        unseen_species_test = data_mat[\"labels\"][test_unseen_loc][0]\n",
        "        unseen_species_test_mapping = {label: i + TRAINING_VALIDATION_SPECIES for i, label in enumerate(np.unique(unseen_species_test))}\n",
        "\n",
        "        assert reduce(np.intersect1d, (seen_species, unseen_species_validation, unseen_species_test)).size == 0\n",
        "\n",
        "        # Union of the two mappings, allows to full remap all the labels\n",
        "        labels_mapping = seen_species_mapping | unseen_species_validation_mapping | unseen_species_test_mapping\n",
        "        assert len(labels_mapping) == NUMBER_OF_SPECIES\n",
        "\n",
        "        species = data_mat[\"labels\"][indeces]  # Consider only train\n",
        "        remapped_labels = np.array([labels_mapping[label.item()] for label in species])\n",
        "        self.remapped_labels = torch.from_numpy(remapped_labels).long()\n",
        "\n",
        "        assert len(torch.unique(self.remapped_labels)) == TRAINING_VALIDATION_SPECIES\n",
        "\n",
        "        # data_mat['G'] returns a ndarray of type uint16, therefore we convert into int16 before invoking from_numpy\n",
        "        self.G = torch.from_numpy(data_mat[\"G\"].astype(np.int16)).long()\n",
        "        self.genera = torch.empty(species.shape).long()\n",
        "        for i in range(indeces.size):\n",
        "            self.genera[i][0] = self.G[species[i][0] - 1][0] - 1041\n",
        "\n",
        "        self.species = data_mat[\"species\"][indeces]\n",
        "        self.ids = data_mat[\"ids\"][indeces]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings_dna)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embedding_img = self.embeddings_img[idx]\n",
        "        embedding_dna = self.embeddings_dna[idx]\n",
        "        label = self.remapped_labels[idx].item()\n",
        "        genera = self.genera[idx].item()\n",
        "\n",
        "        return embedding_img.view(1, -1), embedding_dna.view(1, -1), label, genera\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg3l8IA0saIb"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "fp_osQc7usZm"
      },
      "outputs": [],
      "source": [
        "class AttentionNet(nn.Module):\n",
        "        def __init__(self, num_seen_species, num_genera):\n",
        "                super(AttentionNet, self).__init__()\n",
        "\n",
        "                self.img_fc1 = nn.Linear(2048, 1024)\n",
        "                self.img_fc2 = nn.Linear(1024, 500)\n",
        "\n",
        "                self.inception_encoders_block_1 = InceptionEncodersBlock()\n",
        "\n",
        "                self.fc_species_1 = nn.Linear(4000, 1000)\n",
        "                self.fc_species_2 = nn.Linear(1000, num_seen_species)\n",
        "\n",
        "                self.fc_genera_1 = nn.Linear(4000, 500)\n",
        "                self.fc_genera_2 = nn.Linear(500, num_genera)\n",
        "\n",
        "                self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "        def forward(self, x_img, x_dna):\n",
        "                x_img = F.relu(self.img_fc1(x_img))\n",
        "                x_img = self.img_fc2(x_img)\n",
        "\n",
        "                x_img, x_dna = self.inception_encoders_block_1(x_img, x_dna)\n",
        "\n",
        "                x = torch.cat((x_img, x_dna), axis=1)\n",
        "\n",
        "                x_species = x.clone()\n",
        "                x_genera = x.clone()\n",
        "\n",
        "                x_species = self.dropout(F.relu(self.fc_species_1(x_species)))\n",
        "                x_species = self.fc_species_2(x_species)\n",
        "\n",
        "                x_genera = self.dropout(F.relu(self.fc_genera_1(x_genera)))\n",
        "                x_genera = self.fc_genera_2(x_genera)\n",
        "\n",
        "                return x_species, x_genera\n",
        "\n",
        "class ImageDNAEncoder(nn.Module):\n",
        "        def __init__(self, embed_dim, linear_dim, num_heads):\n",
        "                super(ImageDNAEncoder, self).__init__()\n",
        "                self.multi_head_img_1 = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "                self.multi_head_dna_1 = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "                self.norm_img_1 = nn.LayerNorm(embed_dim)\n",
        "                self.norm_dna_1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "                self.linear_img_1 = nn.Linear(embed_dim, linear_dim)\n",
        "                self.dropout_img = nn.Dropout(0.5)\n",
        "                self.linear_img_2 = nn.Linear(linear_dim, embed_dim)\n",
        "\n",
        "                self.linear_dna_1 = nn.Linear(embed_dim, linear_dim)\n",
        "                self.dropout_dna = nn.Dropout(0.5)\n",
        "                self.linear_dna_2 = nn.Linear(linear_dim, embed_dim)\n",
        "\n",
        "\n",
        "        def forward(self, x_img, x_dna):\n",
        "                identity = x_img\n",
        "                x_img, _ = self.multi_head_img_1(x_img, x_dna, x_dna)\n",
        "                x_img = self.norm_img_1(x_img + identity)\n",
        "                x_img = self.feed_forward_img(x_img)\n",
        "\n",
        "                identity = x_dna\n",
        "                x_dna, _ = self.multi_head_dna_1(x_dna, x_img, x_img)\n",
        "                x_dna = self.norm_dna_1(x_dna + identity)\n",
        "                x_dna = self.feed_forward_dna(x_dna)\n",
        "\n",
        "                return x_img, x_dna\n",
        "\n",
        "        def feed_forward_img(self, x):\n",
        "                return self.linear_img_2(self.dropout_img(F.relu(self.linear_img_1(x))))\n",
        "\n",
        "        def feed_forward_dna(self, x):\n",
        "                return self.linear_dna_2(self.dropout_dna(F.relu(self.linear_dna_1(x))))\n",
        "\n",
        "class InceptionEncodersBlock(nn.Module):\n",
        "        def __init__(self):\n",
        "                super(InceptionEncodersBlock, self).__init__()\n",
        "                self.img_dna_encoder_1 = ImageDNAEncoder(10, 40, 2)\n",
        "                self.img_dna_encoder_2 = ImageDNAEncoder(25, 100, 5)\n",
        "                self.img_dna_encoder_3 = ImageDNAEncoder(50, 200, 5)\n",
        "                self.img_dna_encoder_4 = ImageDNAEncoder(100, 400, 5)\n",
        "                self.img_dna_encoder_5 = ImageDNAEncoder(125, 500, 5)\n",
        "\n",
        "\n",
        "        def forward(self, x_img, x_dna):\n",
        "                '''x_img_1 = torch.reshape(x_img.clone(), (-1, 50, 10))\n",
        "                x_dna_1 = torch.reshape(x_dna.clone(), (-1, 50, 10))\n",
        "                x_img_2 = torch.reshape(x_img.clone(), (-1, 20, 25))\n",
        "                x_dna_2 = torch.reshape(x_dna.clone(), (-1, 20, 25))'''\n",
        "                img_identity = torch.reshape(x_img.clone(), (-1, 500))\n",
        "                dna_identity = torch.reshape(x_dna.clone(), (-1, 500))\n",
        "                x_img_3 = torch.reshape(x_img.clone(), (-1, 10, 50))\n",
        "                x_dna_3 = torch.reshape(x_dna.clone(), (-1, 10, 50))\n",
        "                x_img_4 = torch.reshape(x_img.clone(), (-1, 5, 100))\n",
        "                x_dna_4 = torch.reshape(x_dna.clone(), (-1, 5, 100))\n",
        "                x_img_5 = torch.reshape(x_img.clone(), (-1, 4, 125))\n",
        "                x_dna_5 = torch.reshape(x_dna.clone(), (-1, 4, 125))\n",
        "\n",
        "                '''x_img_1, x_dna_1 = self.img_dna_encoder_1(x_img_1, x_dna_1)\n",
        "                x_img_2, x_dna_2 = self.img_dna_encoder_2(x_img_2, x_dna_2)'''\n",
        "                x_img_3, x_dna_3 = self.img_dna_encoder_3(x_img_3, x_dna_3)\n",
        "                x_img_4, x_dna_4 = self.img_dna_encoder_4(x_img_4, x_dna_4)\n",
        "                x_img_5, x_dna_5 = self.img_dna_encoder_5(x_img_5, x_dna_5)\n",
        "\n",
        "\n",
        "                '''x_img_1 = torch.reshape(x_img_1, (-1, 500))\n",
        "                x_dna_1 = torch.reshape(x_dna_1, (-1, 500))\n",
        "                x_img_2 = torch.reshape(x_img_2, (-1, 500))\n",
        "                x_dna_2 = torch.reshape(x_dna_2, (-1, 500))'''\n",
        "                x_img_3 = torch.reshape(x_img_3, (-1, 500))\n",
        "                x_dna_3 = torch.reshape(x_dna_3, (-1, 500))\n",
        "                x_img_4 = torch.reshape(x_img_4, (-1, 500))\n",
        "                x_dna_4 = torch.reshape(x_dna_4, (-1, 500))\n",
        "                x_img_5 = torch.reshape(x_img_5, (-1, 500))\n",
        "                x_dna_5 = torch.reshape(x_dna_5, (-1, 500))\n",
        "\n",
        "                x_img = img_identity + x_img_3 + x_img_4 + x_img_5\n",
        "                x_dna = dna_identity + x_dna_3 + x_dna_4 + x_dna_5\n",
        "\n",
        "                x_img = torch.concat((img_identity, x_img_3, x_img_4, x_img_5), axis=1)\n",
        "                x_dna = torch.concat((dna_identity, x_dna_3, x_dna_4, x_dna_5), axis=1)\n",
        "\n",
        "                return x_img, x_dna\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEj9iqWHsaId"
      },
      "source": [
        "# Creating datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "Of4Mq0hKsaId"
      },
      "outputs": [],
      "source": [
        "training_set = ImageDNATrainDataset()\n",
        "validation_set = ImageDNAValidationDataset()\n",
        "test_set = ImageDNATestDataset()\n",
        "training_validation_set = ImageDNATrainValidationDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93Uli0msaIe"
      },
      "source": [
        "Defining methods for training, validating and testing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "dm6_gk-JusZo"
      },
      "outputs": [],
      "source": [
        "def train(model, lr, momentum, num_epochs, batch_size, train_val=False, print_losses=False, print_step = 200):\n",
        "    model.train()\n",
        "    criterion_species = torch.nn.CrossEntropyLoss()\n",
        "    criterion_genera = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    if train_val:\n",
        "        loader = torch.utils.data.DataLoader(training_validation_set, batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        running_labels_loss = 0.0\n",
        "        running_genera_loss = 0.0\n",
        "\n",
        "        for i, data in enumerate(loader, 0):\n",
        "            inputs_img, inputs_dna, species, genera = data\n",
        "            inputs_img, inputs_dna, species, genera = inputs_img.to(device), inputs_dna.to(device), species.to(device), genera.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            labels_outputs, genera_outputs = model(inputs_img, inputs_dna)\n",
        "            labels_loss = criterion_species(labels_outputs, species)\n",
        "            genera_loss = criterion_genera(genera_outputs, genera)\n",
        "            total_loss = labels_loss + genera_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print losses\n",
        "            if print_losses:\n",
        "                running_labels_loss += labels_loss.item()\n",
        "                running_genera_loss += genera_loss.item()\n",
        "                if i % print_step == print_step - 1:\n",
        "                    print(f\"[{epoch + 1}, {i + 1:5d}] Species loss: {running_labels_loss / print_step:.3f}; Genera loss: {running_genera_loss / print_step:.3f}\")\n",
        "                    running_labels_loss = 0.0\n",
        "                    running_genera_loss = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "ti1M7rRAusZp"
      },
      "outputs": [],
      "source": [
        "def validate(model, threshold, batch_size):\n",
        "\n",
        "    validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct_predictions_per_labels = defaultdict(int)\n",
        "        total_samples_per_labels = defaultdict(int)\n",
        "        correct_predictions_per_genera = defaultdict(int)\n",
        "        total_samples_per_genera = defaultdict(int)\n",
        "\n",
        "        for data in validation_loader:\n",
        "            inputs_img, inputs_dna, species, genera = data\n",
        "            inputs_img, inputs_dna, species, genera = inputs_img.to(device), inputs_dna.to(device), species.to(device), genera.to(device)\n",
        "\n",
        "            labels_outputs, genera_outputs = model(inputs_img, inputs_dna)\n",
        "\n",
        "            labels_outputs = nn.Softmax(dim=1)(labels_outputs)\n",
        "            genera_outputs = nn.Softmax(dim=1)(genera_outputs)\n",
        "\n",
        "            predicted_labels_values, predicted_labels = torch.topk(labels_outputs.data, k=2, dim=1)\n",
        "            _, predicted_genera = torch.max(genera_outputs.data, 1)\n",
        "\n",
        "            differences = predicted_labels_values[:, 0] - predicted_labels_values[:, 1]\n",
        "            genera_mask = differences <= threshold\n",
        "            labels_mask = ~genera_mask\n",
        "\n",
        "            # Update relative frequencies\n",
        "            for idx in range(len(genera)):\n",
        "                total_samples_per_labels[species[idx].item()] += 1\n",
        "\n",
        "                if labels_mask[idx] and predicted_labels[idx, 0] == species[idx]:\n",
        "                    correct_predictions_per_labels[species[idx].item()] += 1\n",
        "\n",
        "                # if the sample is of one undescribed species\n",
        "                if species[idx].item() not in np.unique(validation_set.seen_species):\n",
        "                    assert genera[idx].item() in np.unique(validation_set.unseen_species_genera)\n",
        "                    total_samples_per_genera[genera[idx].item()] += 1\n",
        "\n",
        "                    if genera_mask[idx] and predicted_genera[idx] == genera[idx]:\n",
        "                        correct_predictions_per_genera[genera[idx].item()] += 1\n",
        "\n",
        "        accuracy_per_label = {label: (correct_predictions_per_labels[label] / total_samples_per_labels[label]) if total_samples_per_labels[label] > 0 else 0 for label in total_samples_per_labels}\n",
        "        accuracy_per_genera = {genera: (correct_predictions_per_genera[genera] / total_samples_per_genera[genera]) if total_samples_per_genera[genera] > 0 else 0 for genera in total_samples_per_genera}\n",
        "\n",
        "        test_described_species_accuracy = 0\n",
        "        for label in np.unique(validation_set.seen_species):\n",
        "            test_described_species_accuracy += accuracy_per_label[label]\n",
        "\n",
        "        test_undescribed_species_accuracy = 0\n",
        "        for genera in np.unique(validation_set.unseen_species_genera):\n",
        "            test_undescribed_species_accuracy += accuracy_per_genera[genera]\n",
        "\n",
        "        normalized_test_described_species_accuracy = test_described_species_accuracy / 629\n",
        "        normalized_test_undescribed_species_accuracy = test_undescribed_species_accuracy / 97\n",
        "\n",
        "        return normalized_test_described_species_accuracy, normalized_test_undescribed_species_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "87_ldlBDusZp"
      },
      "outputs": [],
      "source": [
        "def test(model, threshold, batch_size):\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct_predictions_per_labels = defaultdict(int)\n",
        "        total_samples_per_labels = defaultdict(int)\n",
        "        correct_predictions_per_genera = defaultdict(int)\n",
        "        total_samples_per_genera = defaultdict(int)\n",
        "\n",
        "        for data in test_loader:\n",
        "            inputs_img, inputs_dna, species, genera = data\n",
        "            inputs_img, inputs_dna, species, genera = inputs_img.to(device), inputs_dna.to(device), species.to(device), genera.to(device)\n",
        "\n",
        "            labels_outputs, genera_outputs = model(inputs_img, inputs_dna)\n",
        "\n",
        "            labels_outputs = nn.Softmax(dim=1)(labels_outputs)\n",
        "            genera_outputs = nn.Softmax(dim=1)(genera_outputs)\n",
        "\n",
        "            predicted_labels_values, predicted_labels = torch.topk(labels_outputs.data, k=2, dim=1)\n",
        "            _, predicted_genera = torch.max(genera_outputs.data, 1)\n",
        "\n",
        "            differences = predicted_labels_values[:, 0] - predicted_labels_values[:, 1]\n",
        "            genera_mask = differences <= threshold\n",
        "            labels_mask = ~genera_mask\n",
        "\n",
        "            # Update relative frequencies\n",
        "            for idx in range(len(genera)):\n",
        "                total_samples_per_labels[species[idx].item()] += 1\n",
        "\n",
        "                if labels_mask[idx] and predicted_labels[idx, 0] == species[idx]:\n",
        "                    correct_predictions_per_labels[species[idx].item()] += 1\n",
        "\n",
        "                # if the sample is of one undescribed species\n",
        "                if species[idx].item() not in np.unique(test_set.seen_species):\n",
        "                    assert genera[idx].item() in np.unique(test_set.unseen_species_genera)\n",
        "                    total_samples_per_genera[genera[idx].item()] += 1\n",
        "\n",
        "                    if genera_mask[idx] and predicted_genera[idx] == genera[idx]:\n",
        "                        correct_predictions_per_genera[genera[idx].item()] += 1\n",
        "\n",
        "        accuracy_per_label = {label: (correct_predictions_per_labels[label] / total_samples_per_labels[label]) if total_samples_per_labels[label] > 0 else 0 for label in total_samples_per_labels}\n",
        "        accuracy_per_genera = {genera: (correct_predictions_per_genera[genera] / total_samples_per_genera[genera]) if total_samples_per_genera[genera] > 0 else 0 for genera in total_samples_per_genera}\n",
        "\n",
        "        test_described_species_accuracy = 0\n",
        "        for label in np.unique(test_set.seen_species):\n",
        "            test_described_species_accuracy += accuracy_per_label[label]\n",
        "\n",
        "        test_undescribed_species_accuracy = 0\n",
        "        for genera in np.unique(test_set.unseen_species_genera):\n",
        "            test_undescribed_species_accuracy += accuracy_per_genera[genera]\n",
        "\n",
        "        normalized_test_described_species_accuracy = test_described_species_accuracy / 770\n",
        "        normalized_test_undescribed_species_accuracy = test_undescribed_species_accuracy / 134\n",
        "\n",
        "        return normalized_test_described_species_accuracy, normalized_test_undescribed_species_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zItvqJfusZq",
        "outputId": "86c6e8bb-9703-41ff-e6a1-f04172f9451a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy for lr=0.001, momentum=0.9, num_epochs=10, batch_size=16, threshold=0.9146912192286945: 1.6320135626413064\n",
            "Validation accuracy for lr=0.001, momentum=0.9, num_epochs=10, batch_size=16, threshold=1.0: 0.7134024054460696\n",
            "Validation accuracy for lr=0.001, momentum=0.9, num_epochs=10, batch_size=32, threshold=0.9146912192286945: 0.9885655856955013\n",
            "Validation accuracy for lr=0.001, momentum=0.9, num_epochs=10, batch_size=32, threshold=1.0: 0.7549983108515169\n",
            "Best parameters: {'learning_rate': 0.001, 'momentum': 0.9, 'num_epochs': 10, 'batch_size': 16, 'threshold': 0.9146912192286945}\n"
          ]
        }
      ],
      "source": [
        "lr_values = [0.001]\n",
        "momentum_values = [0.9]\n",
        "batch_size_values = [16, 32]\n",
        "num_epochs_values = [10]\n",
        "threshold_values = np.linspace(0.7, 1, 2)**0.25\n",
        "\n",
        "best_validation_accuracy = 0\n",
        "best_parameters = {}\n",
        "\n",
        "for lr in lr_values:\n",
        "    for momentum in momentum_values:\n",
        "        for num_epochs in num_epochs_values:\n",
        "            for batch_size in batch_size_values:\n",
        "                model = AttentionNet(652, 368)\n",
        "                model.to(device)\n",
        "                train(model, lr, momentum, num_epochs, batch_size)\n",
        "                for threshold in threshold_values:\n",
        "                    validation_species_accuracy, validation_genera_accuracy = validate(model, threshold, batch_size)\n",
        "                    validation_accuracy = validation_species_accuracy + validation_genera_accuracy\n",
        "                    print(f\"Validation accuracy for lr={lr}, momentum={momentum}, num_epochs={num_epochs}, batch_size={batch_size}, threshold={threshold}: {validation_accuracy}\")\n",
        "\n",
        "                    if validation_accuracy > best_validation_accuracy:\n",
        "                        best_validation_accuracy = validation_accuracy\n",
        "                        best_parameters = {'learning_rate': lr, 'momentum': momentum, 'num_epochs': num_epochs, 'batch_size': batch_size, 'threshold': threshold}\n",
        "\n",
        "print(\"Best parameters:\", best_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJgoWU0AusZr",
        "outputId": "69ce2de9-42cf-4a2a-ae95-7aedb040c2d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] Species loss: 6.244; Genera loss: 4.983\n",
            "[1,   400] Species loss: 5.243; Genera loss: 3.643\n",
            "[1,   600] Species loss: 4.621; Genera loss: 3.017\n",
            "[1,   800] Species loss: 4.074; Genera loss: 2.524\n",
            "[1,  1000] Species loss: 3.579; Genera loss: 2.105\n",
            "[1,  1200] Species loss: 3.165; Genera loss: 1.813\n",
            "[2,   200] Species loss: 2.737; Genera loss: 1.583\n",
            "[2,   400] Species loss: 2.481; Genera loss: 1.448\n",
            "[2,   600] Species loss: 2.188; Genera loss: 1.234\n",
            "[2,   800] Species loss: 1.935; Genera loss: 1.074\n",
            "[2,  1000] Species loss: 1.839; Genera loss: 1.048\n",
            "[2,  1200] Species loss: 1.619; Genera loss: 0.925\n",
            "[3,   200] Species loss: 1.384; Genera loss: 0.784\n",
            "[3,   400] Species loss: 1.238; Genera loss: 0.727\n",
            "[3,   600] Species loss: 1.131; Genera loss: 0.678\n",
            "[3,   800] Species loss: 0.966; Genera loss: 0.580\n",
            "[3,  1000] Species loss: 0.827; Genera loss: 0.504\n",
            "[3,  1200] Species loss: 0.741; Genera loss: 0.445\n",
            "[4,   200] Species loss: 0.592; Genera loss: 0.395\n",
            "[4,   400] Species loss: 0.514; Genera loss: 0.356\n",
            "[4,   600] Species loss: 0.451; Genera loss: 0.324\n",
            "[4,   800] Species loss: 0.375; Genera loss: 0.259\n",
            "[4,  1000] Species loss: 0.316; Genera loss: 0.233\n",
            "[4,  1200] Species loss: 0.257; Genera loss: 0.202\n",
            "[5,   200] Species loss: 0.191; Genera loss: 0.163\n",
            "[5,   400] Species loss: 0.180; Genera loss: 0.157\n",
            "[5,   600] Species loss: 0.157; Genera loss: 0.140\n",
            "[5,   800] Species loss: 0.127; Genera loss: 0.117\n",
            "[5,  1000] Species loss: 0.117; Genera loss: 0.104\n",
            "[5,  1200] Species loss: 0.097; Genera loss: 0.100\n",
            "[6,   200] Species loss: 0.085; Genera loss: 0.076\n",
            "[6,   400] Species loss: 0.079; Genera loss: 0.076\n",
            "[6,   600] Species loss: 0.071; Genera loss: 0.076\n",
            "[6,   800] Species loss: 0.076; Genera loss: 0.073\n",
            "[6,  1000] Species loss: 0.062; Genera loss: 0.060\n",
            "[6,  1200] Species loss: 0.047; Genera loss: 0.053\n",
            "[7,   200] Species loss: 0.051; Genera loss: 0.051\n",
            "[7,   400] Species loss: 0.043; Genera loss: 0.044\n",
            "[7,   600] Species loss: 0.045; Genera loss: 0.045\n",
            "[7,   800] Species loss: 0.044; Genera loss: 0.045\n",
            "[7,  1000] Species loss: 0.040; Genera loss: 0.046\n",
            "[7,  1200] Species loss: 0.035; Genera loss: 0.033\n",
            "[8,   200] Species loss: 0.033; Genera loss: 0.037\n",
            "[8,   400] Species loss: 0.025; Genera loss: 0.034\n",
            "[8,   600] Species loss: 0.033; Genera loss: 0.035\n",
            "[8,   800] Species loss: 0.026; Genera loss: 0.031\n",
            "[8,  1000] Species loss: 0.033; Genera loss: 0.033\n",
            "[8,  1200] Species loss: 0.025; Genera loss: 0.031\n",
            "[9,   200] Species loss: 0.020; Genera loss: 0.023\n",
            "[9,   400] Species loss: 0.026; Genera loss: 0.025\n",
            "[9,   600] Species loss: 0.021; Genera loss: 0.026\n",
            "[9,   800] Species loss: 0.021; Genera loss: 0.026\n",
            "[9,  1000] Species loss: 0.020; Genera loss: 0.023\n",
            "[9,  1200] Species loss: 0.019; Genera loss: 0.024\n",
            "[10,   200] Species loss: 0.017; Genera loss: 0.023\n",
            "[10,   400] Species loss: 0.016; Genera loss: 0.021\n",
            "[10,   600] Species loss: 0.021; Genera loss: 0.019\n",
            "[10,   800] Species loss: 0.017; Genera loss: 0.021\n",
            "[10,  1000] Species loss: 0.015; Genera loss: 0.019\n",
            "[10,  1200] Species loss: 0.016; Genera loss: 0.021\n",
            "-------------------------------------------------------------------------------\n",
            "Final model described species accuracy:  0.977125908903506\n",
            "Final model undescribed species accuracy:  0.7073282118461085\n",
            "-------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = AttentionNet(797, 368)\n",
        "model.to(device)\n",
        "train(model, best_parameters['learning_rate'], best_parameters['momentum'], best_parameters['num_epochs'], best_parameters['batch_size'], train_val=True, print_losses=True)\n",
        "species_accuracy, genera_accuracy = test(model, best_parameters['threshold'], best_parameters['batch_size'])\n",
        "\n",
        "print(\"-------------------------------------------------------------------------------\")\n",
        "print(f\"Final model described species accuracy: \", species_accuracy)\n",
        "print(f\"Final model undescribed species accuracy: \", genera_accuracy)\n",
        "print(\"-------------------------------------------------------------------------------\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
